{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**3.lielais mājasdarbs**"
      ],
      "metadata": {
        "id": "dKqyJRYnOjHX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pKHtOTcOgR9"
      },
      "outputs": [],
      "source": [
        "# Autore: Jekaterina Jevtejeva\n",
        "# St.apl.numurs: jj19021\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Datu ielāde no Dropbox\n",
        "!wget '[your link goes here]' -O reviews.csv\n",
        "\n",
        "# Visi dati tiek ielasīti no iegūtā csv faila\n",
        "all_data = pd.read_csv('reviews.csv', delimiter=',', encoding='latin-1', usecols=[1,2,3,4,5,6,7,8,9,10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametri\n",
        "LOAD_MODEL = True # karodziņš, kura True vērtība nozīmē, ka modeli ir jāieladē no faila; False - ka modeli ir jātrenē uz trenēšanas datiem\n",
        "TEST_SET_FILE = 'reviews_test.csv' # testa datu kopas faila nosaukums\n",
        "TRAIN_SET_FILE = 'reviews_train.csv' # trenēšanas datu kopas faila nosaukums\n",
        "STATE_DICT_FILE = 'md3_model_jj19021.pt' # saglabātā modeļa faila nosaukums\n",
        "\n",
        "# Saglabātā modeļa, trenēšanas un testēšanas datu kopu ieguve\n",
        "# Ar wget tiek ielādēti faili no Dropbox\n",
        "!wget 'https://www.dropbox.com/s/vd14ifhf1yb9alb/md3_model_jj19021.pt?dl=0' -O md3_model_jj19021.pt\n",
        "!wget 'https://www.dropbox.com/s/zdy67wcriogc49q/reviews_train.csv?dl=0' -O reviews_train.csv\n",
        "!wget 'https://www.dropbox.com/s/rj5fnz1dtuh7ss0/reviews_test.csv?dl=0' -O reviews_test.csv\n",
        "\n",
        "# Trenēšanas un testēšanas datu ielase no csv failiem\n",
        "train_data = pd.read_csv(TRAIN_SET_FILE, delimiter=',', encoding='latin-1', usecols=[1,2,3,4,5,6,7,8,9,10])\n",
        "test_data = pd.read_csv(TEST_SET_FILE, delimiter=',', encoding='latin-1', usecols=[1,2,3,4,5,6,7,8,9,10])"
      ],
      "metadata": {
        "id": "oO8Ojvc0bMK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ModelLoader klase ar statiskām metodēm modeļa ielādēšanai un saglabāšai failā\n",
        "class ModelLoader():\n",
        "  # Funkcija modeļa ielādēšanai, kas saņem state_dict no faila un ielādē to modelī\n",
        "  @staticmethod\n",
        "  def load_model(model):\n",
        "    state_dict = torch.load(STATE_DICT_FILE)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "  # Funkcija modeļa saglabāšanai, kas iegūst modeļa tekošo state_dict un ielādē to failā\n",
        "  @staticmethod\n",
        "  def save_model(model):\n",
        "    state_dict = model.state_dict()\n",
        "    torch.save(state_dict, STATE_DICT_FILE)"
      ],
      "metadata": {
        "id": "BErneSUjxa5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# DatasetSplitter klase ar statisku metodi datu sadalīšanai atsevišķās datu kopās\n",
        "class DatasetSplitter():\n",
        "  # Funkcija testa un treniņa datu kopu iegūšanai no oriģināla csv faila, sadalot to divos failos\n",
        "  @staticmethod\n",
        "  def split_dataset():\n",
        "    df = pd.read_csv('reviews.csv')\n",
        "    df['split'] = np.random.randn(df.shape[0], 1)\n",
        "    split = np.random.rand(len(df)) <= 0.8\n",
        "    train = df[split]\n",
        "    test = df[~split]\n",
        "    train.to_csv('reviews_train.csv', index=False)\n",
        "    test.to_csv('reviews_test.csv', index=False)\n",
        "\n",
        "    print(len(train))\n",
        "    print(len(test))"
      ],
      "metadata": {
        "id": "5UiTzOhuc9rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Klase TextProcessor, kas atbild par teksta datu apstrādi - datu tīrīšanu un vārdnīcas veidošanu\n",
        "class TextProcessor():\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    self.text_data = self.get_data()[0]\n",
        "    self.vocab = self.get_data()[1]\n",
        "\n",
        "  # Statiska metode datu filtrēšanai\n",
        "  # Izmantotās kolonnas no datu faila ir Title un Review text\n",
        "  @staticmethod\n",
        "  def clean_up(raw_data):\n",
        "    pattern = re.compile(\"[^a-zA-Z ]+\") # notīra punktiāciju\n",
        "    raw_data[\"Title\"] = raw_data[\"Title\"].astype(str).map(lambda x: pattern.sub('', x)).str.lower()\n",
        "    raw_data[\"Review Text\"] = raw_data[\"Review Text\"].astype(str).map(lambda x: pattern.sub('', x)).str.lower()\n",
        "    raw_data = raw_data[raw_data[\"Title\"].astype(str).map(len) > 5] # atstāj tikai rindas ar garumu lielāku par 5\n",
        "    clean = raw_data[raw_data[\"Review Text\"].astype(str).map(len) > 5]\n",
        "    return clean\n",
        "\n",
        "  # Tokenizācija un vārdnīcas iegūšana\n",
        "  # Koda fragments paņemts no Google Colab piemēra \"Laboratorija Spam PyTorch\"\n",
        "  def get_data(self):\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "    titles = self.data[\"Title\"].astype(str)\n",
        "    reviews = self.data[\"Review Text\"].astype(str)\n",
        "    input_text = [tokenizer(sentence) for sentence in titles + reviews]\n",
        "    vocab = build_vocab_from_iterator(iter(input_text), specials=[\"<unk>\",\"<pad>\"], max_tokens=3000) # ņemam 3000 biežākos vārdus\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    input_text = [torch.tensor(vocab(tokens)) for tokens in input_text]\n",
        "    input_text = torch.nn.utils.rnn.pad_sequence(input_text, padding_value=vocab['<pad>'], batch_first=True)\n",
        "    return input_text, vocab\n",
        "\n",
        "clean_data = TextProcessor.clean_up(all_data) # nofiltrēti dati\n",
        "text_processor = TextProcessor(clean_data)\n",
        "vocabulary = text_processor.vocab # iegūtā vārdnīca\n",
        "text_data = text_processor.text_data # iegūtie tekstuālie dati\n",
        "print(len(vocabulary))"
      ],
      "metadata": {
        "id": "00ul9nLf8iBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "BATCH_SIZE = 64 # parametrs, kas nosaka batch_size priekš DataLoaders\n",
        "\n",
        "# Dataset realizācija\n",
        "class ReviewsDataset(Dataset):\n",
        "  def __init__(self, dataset, text_data):\n",
        "    self.dataset = dataset # dati\n",
        "    self.text_data = text_data # tekstuāli dati\n",
        "    self.num_data = self.get_num_data() # skaitliski dati\n",
        "    self.answers1 = self.get_rec_labels() # pareizās atbildes priekš Recommended ID\n",
        "    self.answers2 = self.get_rating_labels() # pareizās atbildes priekš Rating\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    return self.text_data[key], self.num_data[key], self.answers1[key], self.answers2[key]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def get_num_data(self):\n",
        "    input_num_data = self.dataset[\"Positive Feedback Count\"].astype('int')\n",
        "    input_num_data = torch.Tensor(input_num_data.values)\n",
        "    input_num_data = torch.tensor(input_num_data, dtype=torch.int64)\n",
        "    return input_num_data\n",
        "\n",
        "  def get_rec_labels(self):\n",
        "    recommended = self.dataset[\"Recommended IND\"].values\n",
        "    return torch.tensor(recommended, dtype=torch.float32)\n",
        "\n",
        "  def get_rating_labels(self):\n",
        "    rating = self.dataset[\"Rating\"].astype('int').values\n",
        "    return torch.tensor(rating, dtype=torch.float32)\n",
        "\n",
        "# Nofiltrētas datu kopas\n",
        "clean_train_data = TextProcessor.clean_up(train_data)\n",
        "clean_test_data = TextProcessor.clean_up(test_data)\n",
        "\n",
        "# Trenēšanas un testēšanas ReviewsDataset datu kopas un to DataLoaders\n",
        "train_dataset = ReviewsDataset(clean_train_data, text_data)\n",
        "test_dataset = ReviewsDataset(clean_test_data, text_data)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "FfrIr5Eg_-4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tīkla parametri\n",
        "# Tika noskaidroti, vairākkārt testējot modeli un skatoties uz iegūto precizitāti\n",
        "EMBEDDING_INIT = len(vocabulary)\n",
        "EMBEDDING_SIZE = 256\n",
        "HIDDEN_SIZE_FST = 128\n",
        "HIDDEN_SIZE_SND = 64\n",
        "NUM_OUT_FEATURES = 32\n",
        "REC_OUT_FEATURES = 1\n",
        "RATING_OUT_FEATURES = 6\n",
        "\n",
        "# Tīkla modulis\n",
        "# Slāņi tika noskaidroti, vairākkārt testējot modeli un skatoties uz iegūto precizitāti,\n",
        "# kā piemēru sākotnēji lietojot MNIST modeļa realizāciju no Google Colab piemēra \"Laboratorija Spam PyTorch\"\n",
        "class ReviewsModel(nn.Sequential):\n",
        "  def __init__(self):\n",
        "    super(ReviewsModel, self).__init__()\n",
        "    self.text_layers = nn.Sequential(\n",
        "        nn.Embedding(EMBEDDING_INIT, EMBEDDING_SIZE),\n",
        "        nn.LSTM(input_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE_FST),\n",
        "        extract_tensor(),\n",
        "        # nn.Dropout(0.25),\n",
        "        nn.LSTM(input_size=HIDDEN_SIZE_FST, hidden_size=HIDDEN_SIZE_SND),\n",
        "        extract_tensor(),\n",
        "        # nn.Dropout(0.25),\n",
        "        nn.Flatten(),\n",
        "    )\n",
        "    self.num_layers = nn.Sequential(\n",
        "        nn.LazyLinear(out_features=NUM_OUT_FEATURES),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.rec_layers = nn.Sequential(\n",
        "        nn.Linear(in_features=NUM_OUT_FEATURES, out_features=REC_OUT_FEATURES),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "    self.rating_layers = nn.Sequential(\n",
        "        nn.Linear(in_features=NUM_OUT_FEATURES, out_features=RATING_OUT_FEATURES),\n",
        "        nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    x = self.text_layers(x)\n",
        "    y = torch.cat((x.view(x.size(0), -1), y.view(y.size(0), -1)), dim=1)\n",
        "    y = self.num_layers(y)\n",
        "    recommendation = self.rec_layers(y)\n",
        "    recommendation = torch.squeeze(recommendation, 1)\n",
        "    rating = self.rating_layers(y)\n",
        "    return recommendation, rating\n",
        "\n",
        "# LSTM() atgriež tuple no (tensor, (recurrent state))\n",
        "# Koda fragments paņemts no Google Colab piemēra \"Laboratorija Spam PyTorch\"\n",
        "class extract_tensor(nn.Module):\n",
        "    def forward(self, x):\n",
        "        tensor, _ = x\n",
        "        return tensor\n",
        "\n",
        "model = ReviewsModel()"
      ],
      "metadata": {
        "id": "d07TudnhAEwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "lr = 0.01\n",
        "\n",
        "# GPU pieejamības pārbaude\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "# BCEWithLogitsLoss tika izvēlēts, jo to lieto priekš binary classification: Eg 1. x=[-2.34] < 0 → class 0; Eg 2. x=[3.87] > 0 → class 1\n",
        "criterion1 = nn.BCEWithLogitsLoss()\n",
        "# CrossEntropyLoss tika izvēlēts, jo to lieto priekš multi-class classification: Eg. x=[-1.33, 3.89], Argmax(x) → class 1\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "5pAiC06IrBNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Ja modeļa ielādes karodziņš ir patiess, ielādē modeļa state_dict no faila\n",
        "if LOAD_MODEL:\n",
        "  ModelLoader.load_model(model)\n",
        "# Citādi notiek modeļa trenēšana\n",
        "# Koda fragments paņemts no Google Colab piemēra \"Laboratorija Spam PyTorch\"\n",
        "else:\n",
        "  # Trenēšana\n",
        "  for epoch in range(epochs):\n",
        "      recommended_loss = 0.0\n",
        "      rating_loss = 0.0\n",
        "      running_loss = 0.0\n",
        "\n",
        "      for times, rows in enumerate(train_loader):\n",
        "          input_text, input_num, labels_rec, labels_rating = rows[0].to(device), rows[1].to(device), rows[2].to(device), rows[3].to(device)\n",
        "\n",
        "          # Notīram gradientus\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Foward + backward + optimize\n",
        "          out1, out2 = model(input_text, input_num)\n",
        "\n",
        "          loss1 = criterion1(out1, labels_rec) # rekomendāciju zudumfunkcija\n",
        "          loss2 = criterion2(out2, labels_rating.long()) # reitingu zudumfunkcija\n",
        "          loss = loss1 + loss2\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          # Izdrukājam statistiku\n",
        "          recommended_loss += loss1.item()\n",
        "          rating_loss += loss2.item()\n",
        "          running_loss += loss.item()\n",
        "          if times % 1000 == 999 or times+1 == len(train_loader):\n",
        "              print('[%d/%d, %d/%d] Recommended loss: %.3f | Rating loss: %.3f | Running loss: %.3f' % (epoch+1, epochs, times+1, len(train_loader), loss1/times, loss2/times, loss/times))\n",
        "                # running_loss / times - vidējā zudumfunkcija tekošā epohā\n",
        "\n",
        "  print('Training Finished.')"
      ],
      "metadata": {
        "id": "kRLEtbDyry-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeļa saglabāšana\n",
        "ModelLoader.save_model(model)"
      ],
      "metadata": {
        "id": "tWHEITdQxVzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testēšana un rezultāti\n",
        "# Tiek izmantota testa datu kopa\n",
        "correct_rec = 0\n",
        "correct_rating = 0\n",
        "total_recs = 0\n",
        "total_ratings = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        input_text, input_num, labels1, labels2 = data[0].to(device), data[1].to(device), data[2].to(device), data[3].to(device)\n",
        "        labels_rec = labels1.type(torch.FloatTensor)\n",
        "        labels_rating = labels2.type(torch.FloatTensor)\n",
        "\n",
        "        out1, out2 = model(input_text, input_num)\n",
        "        out1_rounded = torch.round(out1)\n",
        "        _, out2_max = torch.max(out2, 1)\n",
        "\n",
        "        total_recs += labels_rec.size(0)\n",
        "        total_ratings += labels_rating.size(0)\n",
        "        correct_rec += (out1_rounded == labels_rec).sum().item() # pareizās Recommended atbildes\n",
        "        correct_rating += (out2_max == labels_rating).sum().item() # pareizās Rating atbildes\n",
        "\n",
        "print('\"Recommended IND\" precizitāte uz testa datu kopas: %.2f %%' % (100*correct_rec / total_recs))\n",
        "print('\"Rating\" precizitāte uz testa datu kopas: %.2f %%' % (100*correct_rating / total_ratings))\n",
        "\n",
        "# Autore: Jekaterina Jevtejeva\n",
        "# St.apl.numurs: jj19021"
      ],
      "metadata": {
        "id": "e8vNyn37V2Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6c3f1a-db18-4501-f837-657fe5a7a644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Recommended IND\" precizitāte uz testa datu kopas: 81.46 %\n",
            "\"Rating\" precizitāte uz testa datu kopas: 52.05 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nBK4jgrps4lv"
      }
    }
  ]
}
